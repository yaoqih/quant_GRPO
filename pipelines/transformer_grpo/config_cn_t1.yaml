qlib:
  provider_uri: ./data/qlib_1d_c
  region: cn
  expression_cache: null
  dataset_cache: null

data:
  handler:
    class: Alpha158
    module_path: qlib.contrib.data.handler
    kwargs:
      instruments: all
      start_time: 2019-01-01
      end_time: 2025-01-01
      fit_start_time: 2019-01-01
      fit_end_time: 2025-12-31
      freq: day
      infer_processors:
        - class: RobustZScoreNorm
          kwargs:
            fields_group: feature
            clip_outlier: true
        - class: Fillna
          kwargs:
            fields_group: feature
      learn_processors:
        - class: DropnaLabel
      label:
        - "Ref($open, -2) / Ref($open, -1) - 1"
  segments:
    train: ["2019-01-01", "2022-12-31"]
    valid: ["2023-01-01", "2023-12-31"]
    test: ["2024-01-01", "2024-12-31"]
  feature_group: feature
  label_group: label
  feature_dtype: float32
  min_instruments: 30
  max_instruments: 3000
  reward:
    clip: [-2.0, 2.0]
    scale: 10.0
    normalize:
      eps: 1.0e-06
  augment:
    temporal_span: 8
    include_cash_token: true
    cash_return: 0.0
    cash_token: "CASH"
    cache:
      enable: true
      root: runs/daily_batch_cache
      compress: true
      feature_dtype: float32
      reuse_existing: true
      force_refresh: false
      # num_workers removed - using synchronous cache writes for lower memory

model:
  d_model: 128
  nhead: 2
  num_layers: 2
  dropout: 0.4
  ff_multiplier: 4
  temporal_nhead: 4
  temporal_layers: 2
  noise_std: 0.02

training:
  seed: 42
  device: auto
  epochs: 40
  batch_size: 8
  learning_rate: 0.0003
  weight_decay: 0.01
  entropy_coef: 0.001
  rl_objective: sharpe   # sharpe | dpr | hybrid
  downside_lambda: 0.5
  sharpe_eps: 1.0e-04
  rl_checkpoint_path: null   # set to a ckpt to skip pretrain and start RL-only
  load_rl_optimizer: false
  rank_coef: 0.05
  rank_coef_final: 0.0
  rank_coef_decay_epochs: 10
  turnover_cost: 0.001
  grad_clip: 1.0
  grad_accum_steps: 24
  early_stop_patience: 20
  pretrain_epochs: 20
  pretrain_early_stop_patience: 5   # 0 disables valid-based early stop
  continue_rl_after_pretrain: false
  checkpoint_root: runs/transformer_grpo
  log_interval: 50
  use_cosine_lr: false
  temperature: 1.0
  train_top_k: 1
  rl_top_k: 1
  rl_temperature: 0.2

  pretrain_loss_weights:
    listmle: 1.2
    ic: 1.0
  pretrain_entropy_coef: 0.003
  pretrain_listmle_warmup_steps: 2000
  feature_l1_coef: 1.0e-05
  rank_loss_type: ic

  dataloader:
    num_workers: 16
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 8
    prefetch_batches: 16
    async_device_transfer: true

  logger:
    type: wandb
    project: transformer_grpo
    mode: online
    split_pretrain_run: false

backtest:
  segment: test
  commission: 0.0
  slippage: 0.0
  top_k: 1
  min_weight: 0.02
  temperature: 0.5
